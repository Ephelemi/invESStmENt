{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1da0ca",
   "metadata": {},
   "source": [
    "## Aufgabe 6: Machine Learning for NLP - Überblick\n",
    "Unsere heutige Übung soll einen ersten, groben Überblick über Machine Learning bieten.\n",
    "Für Machine Learning gibt es umfassende Unterstützung wie z. B. [scikit-learn](https://scikit-learn.org/stable/getting_started.html). \n",
    "\n",
    "Zunächst starten wir mit einer Exkursion zum Thema *Bag-of-Words* bevor wir detaillierter den Ablauf einer Klassifikation angehen. \n",
    "\n",
    "#### Benötigte Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77413853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866ecca8",
   "metadata": {},
   "source": [
    "### Exkursion: Bag-of-Words\n",
    "#### Vokabular\n",
    "Wir starten zunächst mit einem kleinen Beispiel, um die Grundlagen besser zu verstehen. \n",
    "Die drei Sätze in der List *texts* entsprechen unseren Dokumenten und bilden gemeinsam das gesamte Korpus. \n",
    "Insgesamt enthalten die drei Sätze 12 verschiedenen Wörter. \n",
    "Somit haben wir 12 verschiedene Wörter in unserem gesamten Korpus, woraus wiederum folgt, dass das Vokabular 12 Wörter enthält. \n",
    "Nach der Transformierung wird jedes Dokument durch einen Vektor der Größe 12 dargestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d5d5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"blue car and blue window\",\n",
    "    \"black crow in the window\",\n",
    "    \"i see my reflection in the window\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f4b34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ['and', 'black', 'blue', 'car', 'crow', 'i', 'in', 'my', 'reflection', 'see', 'the', 'window']\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary\n",
    "vocab = sorted(set(word for sentence in texts for word in sentence.split()))\n",
    "print(len(vocab), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348d805",
   "metadata": {},
   "source": [
    "##### Binärmatrix\n",
    "Mit dem [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) und der Option 'binary=True' wird eine Binärmatrix erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9848b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'black', 'blue', 'car', 'crow', 'in', 'my', 'reflection', 'see', 'the', 'window']\n"
     ]
    }
   ],
   "source": [
    "# create CountVectorizer\n",
    "bin_vec = CountVectorizer(binary=True)\n",
    "# fit vectorizer on data\n",
    "bin_vec.fit(texts)\n",
    "\n",
    "print([w for w in sorted(bin_vec.vocabulary_.keys())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d44a65",
   "metadata": {},
   "source": [
    "Das Vokabular kann in einen [pandas-DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) eingelesen und dargestellt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d41e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>car</th>\n",
       "      <th>crow</th>\n",
       "      <th>in</th>\n",
       "      <th>my</th>\n",
       "      <th>reflection</th>\n",
       "      <th>see</th>\n",
       "      <th>the</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  black  blue  car  crow  in  my  reflection  see  the  window\n",
       "0    1      0     1    1     0   0   0           0    0    0       1\n",
       "1    0      1     0    0     1   1   0           0    0    1       1\n",
       "2    0      0     0    0     0   1   1           1    1    1       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bin_vec.transform(texts).toarray(), columns=sorted(bin_vec.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23566631",
   "metadata": {},
   "source": [
    "##### Frequenzmatrix\n",
    "Setzt man die Option 'binary' nicht auf True, dann erstellt der [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) eine Frequenzmatrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96760f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>car</th>\n",
       "      <th>crow</th>\n",
       "      <th>in</th>\n",
       "      <th>my</th>\n",
       "      <th>reflection</th>\n",
       "      <th>see</th>\n",
       "      <th>the</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  black  blue  car  crow  in  my  reflection  see  the  window\n",
       "0    1      0     2    1     0   0   0           0    0    0       1\n",
       "1    0      1     0    0     1   1   0           0    0    1       1\n",
       "2    0      0     0    0     0   1   1           1    1    1       1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "cnt_vec.fit(texts)\n",
    "\n",
    "pd.DataFrame(cnt_vec.transform(texts).toarray(), columns=sorted(cnt_vec.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c780e6f",
   "metadata": {},
   "source": [
    "#### tf-idf-Matrix\n",
    "Der [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) erstellt eine Matrix basierend auf tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e27afd06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>black</th>\n",
       "      <th>blue</th>\n",
       "      <th>car</th>\n",
       "      <th>crow</th>\n",
       "      <th>in</th>\n",
       "      <th>my</th>\n",
       "      <th>reflection</th>\n",
       "      <th>see</th>\n",
       "      <th>the</th>\n",
       "      <th>window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.396875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.793749</td>\n",
       "      <td>0.396875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534093</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.534093</td>\n",
       "      <td>0.406192</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.406192</td>\n",
       "      <td>0.315444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358291</td>\n",
       "      <td>0.47111</td>\n",
       "      <td>0.47111</td>\n",
       "      <td>0.47111</td>\n",
       "      <td>0.358291</td>\n",
       "      <td>0.278245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and     black      blue       car      crow        in       my  \\\n",
       "0  0.396875  0.000000  0.793749  0.396875  0.000000  0.000000  0.00000   \n",
       "1  0.000000  0.534093  0.000000  0.000000  0.534093  0.406192  0.00000   \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.358291  0.47111   \n",
       "\n",
       "   reflection      see       the    window  \n",
       "0     0.00000  0.00000  0.000000  0.234400  \n",
       "1     0.00000  0.00000  0.406192  0.315444  \n",
       "2     0.47111  0.47111  0.358291  0.278245  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_vec.fit(texts)\n",
    "\n",
    "pd.DataFrame(tfidf_vec.transform(texts).toarray(), columns=sorted(tfidf_vec.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe57938",
   "metadata": {},
   "source": [
    "### a) Ablauf einer Klassifikation \n",
    "Den Use Case nehmen wir aus der NLP-Anwendung *Sentiment Analysis*. Die Datei *wine_reviews.csv.txt* enthält Rezensionen sowie ein Rating (terrific | horrible) zu Weinen. Wir werden ein Modell trainieren, das uns voraussagt, ob eine Rezension *terrific* oder *horrible* ist.\n",
    "\n",
    "1. Preprocessing\n",
    "2. Feature Extraction\n",
    "3. *k*-fold Cross Validation\n",
    "4. Hyperparameter-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4611036a",
   "metadata": {},
   "source": [
    "#### 1. Preprocessing Pipeline\n",
    "Wie in der Vorlesung erläutert, sollten die Daten vor Machine Learning-Anwendungen bereinigt werden. Basierend auf unserem Use Case *Sentiment Analysis* führen wir folgende Preprocessing-Schritte aus:\n",
    "\n",
    "- Kontraktionen expandieren (wegen Negationen)\n",
    "- Tokenization & Lemmatization \n",
    "- Stopwords entfernen (außer Negationen)\n",
    "- Satzzeichen entfernen\n",
    "\n",
    "##### 1.1 spaCy-Modell, Stoppwort-Liste und Satzzeichen-Liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b818c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# English spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Stop word list from NLTK\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "# Punctuation list from string\n",
    "puncts = string.punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20a3a7",
   "metadata": {},
   "source": [
    "##### 1.2 Kontraktionen expandieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84cb2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kontraktionen exandieren -> Negation\n",
    "def decontract_phrase(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"shan\\'t\", \"shall not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not \", phrase)\n",
    "    phrase = re.sub(r\"\\'re \", \" are \", phrase)\n",
    "    phrase = re.sub(r\"\\'s \", \" is \", phrase)\n",
    "    phrase = re.sub(r\"\\'d \", \" would \", phrase)\n",
    "    phrase = re.sub(r\"\\'ll \", \" will \", phrase)\n",
    "    phrase = re.sub(r\"\\'t \", \" not \", phrase)\n",
    "    phrase = re.sub(r\"\\'ve \", \" have \", phrase)\n",
    "    phrase = re.sub(r\"\\'m \", \" am \", phrase)\n",
    "    \n",
    "    phrase = re.sub(r'\\s+', ' ', phrase)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe399ef2",
   "metadata": {},
   "source": [
    "##### 1.3 Tokenisierung und Lemmatisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d7a6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(review):\n",
    "    doc = nlp(review)\n",
    "    lemma_text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return lemma_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d750d",
   "metadata": {},
   "source": [
    "##### 1.4 Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab0f1c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine not like wine'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_pipeline(review):\n",
    "    # Kontraktionen expandieren\n",
    "    review = decontract_phrase(review)\n",
    "    # Tokenisierung und Lemmatisierung\n",
    "    review = get_lemma(review)    \n",
    "    # Stopwords\n",
    "    review = ' '.join([token for token in review.split() if token.lower() not in stopword_list])\n",
    "    # Satzzeichen\n",
    "    review = ''.join([character for character in review if character not in puncts])\n",
    "    # Remove multiple whitespaces\n",
    "    review = re.sub(r'^\\s+', '', review)\n",
    "    review = re.sub(r' +', ' ', review)\n",
    "    review = re.sub(r'\\s+$', '', review)\n",
    "    \n",
    "    return review\n",
    "\n",
    "test = \"This wine isn't like other wines.\"\n",
    "preprocess_pipeline(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac60ec",
   "metadata": {},
   "source": [
    "##### 1.5 Datei mittels pandas einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e91ae779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Liquorice, cherry fruit. Simple and coarse at ...</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thin and completely uninspiring.</td>\n",
       "      <td>horrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charming, violet-fragranced nose. Classic Guig...</td>\n",
       "      <td>terrific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good grip and a touch of rusticity on the leng...</td>\n",
       "      <td>terrific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gummy nose, merlot-style sweetness. Good wine,...</td>\n",
       "      <td>terrific</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review    Rating\n",
       "0  Liquorice, cherry fruit. Simple and coarse at ...  horrible\n",
       "1                   Thin and completely uninspiring.  horrible\n",
       "2  Charming, violet-fragranced nose. Classic Guig...  terrific\n",
       "3  Good grip and a touch of rusticity on the leng...  terrific\n",
       "4  Gummy nose, merlot-style sweetness. Good wine,...  terrific"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./wine_reviews.csv.txt')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4d04334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 532 entries, 0 to 531\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Review  532 non-null    object\n",
      " 1   Rating  532 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Are all columns defined?\n",
    "# Non-Null Count shows how many rows for each column are well-defined\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923b75a",
   "metadata": {},
   "source": [
    "##### 1.6 Preprocessing anwenden\n",
    "Preprocessing auf Spalte 'Review' anwenden und Ergebnis in einer neuen Spalte 'Preprocessed' im DataFrame  speichern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8858b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Preprocessed'] = dataset['Review'].apply(preprocess_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09b76e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 532 entries, 0 to 531\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Review        532 non-null    object\n",
      " 1   Rating        532 non-null    object\n",
      " 2   Preprocessed  532 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b15ef",
   "metadata": {},
   "source": [
    "#### 2. Feature Extraction\n",
    "\n",
    "##### 2.1 Features\n",
    "Wir erstellen für unsere Reviews eine Frequenzmatrix auf Basis von Unigrammen mit Hilfe des [CountVectorizers](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). \n",
    "\n",
    "**Hinweis:** \n",
    "- Vectorizer werden ausschließlich auf Text angewendet\n",
    "- Als Ergebnis erhält man Features\n",
    "- Diese Features entsprechen X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a341519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level CountVectorizer\n",
    "cnt_vec = CountVectorizer()\n",
    "X_cnt = cnt_vec.fit_transform(dataset['Preprocessed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df8312f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 1507)\n"
     ]
    }
   ],
   "source": [
    "print(X_cnt.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad06e27",
   "metadata": {},
   "source": [
    "##### 2.2 Linguistic Features\n",
    "Zusätzlich zu den oben erstellten Features können wir z. B. zusätzlich linguistische Features nutzen. Diese müssen aber normalisiert werden (siehe Vorlesung). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ed6d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['char_count'] = dataset['Review'].apply(len)\n",
    "dataset['word_count'] = dataset['Review'].apply(lambda x: len(x.split()))\n",
    "dataset['Density'] = dataset['char_count'] / (dataset['word_count']+1)\n",
    "dataset['punctuation_count'] = dataset['Review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "dataset['Punct_Count_Ratio'] = dataset['punctuation_count'] / dataset['word_count']\n",
    "\n",
    "# Beispiele für andere linguistische Feature\n",
    "#dataset['title_word_count'] = dataset['Review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "#dataset['upper_case_word_count'] = dataset['Review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ae72a2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 532 entries, 0 to 531\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Review             532 non-null    object \n",
      " 1   Rating             532 non-null    object \n",
      " 2   Preprocessed       532 non-null    object \n",
      " 3   char_count         532 non-null    int64  \n",
      " 4   word_count         532 non-null    int64  \n",
      " 5   Density            532 non-null    float64\n",
      " 6   punctuation_count  532 non-null    int64  \n",
      " 7   Punct_Count_Ratio  532 non-null    float64\n",
      "dtypes: float64(2), int64(3), object(3)\n",
      "memory usage: 33.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899a8f3",
   "metadata": {},
   "source": [
    "##### 2.3 Data Normalization\n",
    "Die unter 2.2 generierten, linguistischen Features müssen normalisiert werden, d. h. es muss eine Abbildung von numerischen Variablen auf eine gemeinsame Skala stattfinden. \n",
    "\n",
    "Density- und Punct_Count_Ratio-Werte werden mittels [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) umskaliert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f0efe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 2)\n"
     ]
    }
   ],
   "source": [
    "X_dense = X_cnt.toarray()\n",
    "\n",
    "X_ling = np.vstack((dataset['Density'], \n",
    "                    dataset['Punct_Count_Ratio'])).T\n",
    "print(X_ling.shape)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_ling_scale = scaler.fit_transform(X_ling)\n",
    "X = np.hstack((X_dense, X_ling_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83186d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 1509)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376d235",
   "metadata": {},
   "source": [
    "##### 2.4 Labels kodieren\n",
    "Labels, die nicht als Zahlen repräsentiert sind, müssen mittels [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) umgewandelt werden.\n",
    "\n",
    "**Hinweis:**\n",
    "- LabelEncoder wird ausschließlich auf Labels angewendet\n",
    "- Als Ergebnis erhält man eine eindeutige Zahl für jedes Label\n",
    "- Dies entspricht Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b2095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(dataset['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275aae59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532,)\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc6eb5",
   "metadata": {},
   "source": [
    "#### 3. *k*-fold Cross Validation\n",
    "\n",
    "Ablauf der Cross Validation:\n",
    "1. Klassifikator mit *k-fold cross validation* trainieren\n",
    "2. Evaluierung\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85036c94",
   "metadata": {},
   "source": [
    "##### 3.1  *k*-fold Cross Validation\n",
    "Nachdem alle Vorbereitungen getroffen sind, können wir ein Modell trainieren. Anstatt einen Teil der Daten als Testset zu benutzen, werden wir die Performanz auf dem gesamten Datensatz mittels *k-fold cross validation* ermitteln. In *scikit-learn* wird dies mittels [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html?highlight=stratifiedkfold#sklearn.model_selection.StratifiedKFold) bewerkstelligt. Wir setzen *k = 5* und nehmen eine [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) als Klassifikator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cddd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 done\n",
      "Fold 2 done\n",
      "Fold 3 done\n",
      "Fold 4 done\n",
      "Fold 5 done\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "evaluation_list = list()\n",
    "\n",
    "for fold, [train, test] in enumerate(kfold.split(X, y)):\n",
    "    # Get test data\n",
    "    X_test = X[test]\n",
    "    y_test = y[test]\n",
    "    # Define model\n",
    "    model = svm.SVC()\n",
    "    # Fit model\n",
    "    model.fit(X[train], y[train])\n",
    "    # Predict test set with model\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate model\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 4)\n",
    "    precision = round(precision_score(y_test, y_pred), 4)\n",
    "    recall = round(recall_score(y_test, y_pred), 4)\n",
    "    f1 = round(f1_score(y_test, y_pred), 4)\n",
    "    # Append results\n",
    "    evaluation_list.append([fold+1, accuracy, precision, recall, f1])\n",
    "    print('Fold', fold+1, 'done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d432b31",
   "metadata": {},
   "source": [
    "##### 3.2 Evaluierung\n",
    "Im letzten Schritt lassen wir uns Accuracy, Precision, Recall und macro-averaged F1 für jeden Fold sowie den Durchschnitt dieser Werte ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed69ee13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold  Accuracy  Precision  Recall  F1-Score\n",
      "0    1    0.7850     0.7843  0.7692    0.7767\n",
      "1    2    0.8037     0.8605  0.7115    0.7789\n",
      "2    3    0.8019     0.8000  0.7843    0.7921\n",
      "3    4    0.8396     0.8400  0.8235    0.8317\n",
      "4    5    0.8679     0.9524  0.7692    0.8511\n",
      "5  AVG    0.8196     0.8474  0.7715    0.8061\n"
     ]
    }
   ],
   "source": [
    "# Export Results\n",
    "acc_avg = [i[1] for i in evaluation_list]\n",
    "prec_avg = [i[2] for i in evaluation_list]\n",
    "recall_avg = [i[3] for i in evaluation_list]\n",
    "f1_avg = [i[4] for i in evaluation_list]\n",
    "\n",
    "evaluation_list.append(['AVG',\n",
    "                        round(np.mean(acc_avg), 4),\n",
    "                        round(np.mean(prec_avg), 4),\n",
    "                        round(np.mean(recall_avg), 4),\n",
    "                        round(np.mean(f1_avg), 4)])\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_list, columns=['Fold', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "#evaluation_df.to_csv('./Wine_SVM_evaluation.csv', index=False, sep=';')\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c2677",
   "metadata": {},
   "source": [
    "#### 4. Hyperparameter-Tuning \n",
    "Das Hyperparameter-Tuning ist eine andere Möglichkeit, bessere Ergebnisse zu bekommen. \n",
    "Viele Algorithmen verfügen über interne Parameter, die angepasst werden können.\n",
    "\n",
    "In scikit-learn gibt es zwei Möglichkeiten:\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html): The grid search provided by GridSearchCV exhaustively generates candidates from a grid of parameter values specified with the param_grid parameter.\n",
    "- [Randomized CV Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html?highlight=randomized#sklearn.model_selection.RandomizedSearchCV): This implements a randomized search over parameters, where each setting is sampled from a distribution over possible parameter values. This has two main benefits over an exhaustive search:\n",
    "    - A budget can be chosen independent of the number of parameters and possible values.\n",
    "    - Adding parameters that do not influence the performance does not decrease efficiency.\n",
    "\n",
    "**Ablauf des Hyperparameter-Tunings:**\n",
    "1. Datensatz in Trainings- und Testdaten teilen\n",
    "2. Parameter-Grid erstellen\n",
    "3. GridSearch mit SVM\n",
    "4. Ausgabe der besten Parameter \n",
    "5. Modell mit besten Parameter trainieren und evaluieren\n",
    "\n",
    "Als Features und Labels nutzen wir die bereits erstellten Vektoren. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b95c7a",
   "metadata": {},
   "source": [
    "##### 4.1 Datensatz in Trainings- und Testdaten teilen\n",
    "Für das Hyperparameter-Tuning erstellen wir manuell ein Trainings- und Testset mit [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train%20test%20split#sklearn.model_selection.train_test_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ed610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b7961a",
   "metadata": {},
   "source": [
    "##### 4.2 Parameter-Grid erstellen\n",
    "Erstellung des Suchraum für die Parameter <code>C</code> und <code>gamma</code>. Dazu lassen wir uns mit der [logspace-Funktion](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html) von *numpy* verschiedene Werte generieren und erstellen mit diesen den Paramter-Grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2458b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba600b0",
   "metadata": {},
   "source": [
    "##### 4.3 GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a7323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 169 candidates, totalling 845 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': array([1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03, 1.e+04, 1.e+05,\n",
       "       1.e+06, 1.e+07, 1.e+08, 1.e+09, 1.e+10]),\n",
       "                         'gamma': array([1.e-09, 1.e-08, 1.e-07, 1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02,\n",
       "       1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03])},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now create a GridSearchCV object and fit it to the data\n",
    "search = GridSearchCV(estimator=svm.SVC(),\n",
    "                      param_grid=param_grid,\n",
    "                      verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d4234",
   "metadata": {},
   "source": [
    "##### 4.4 Ausgabe der besten Parameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281607f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'C': 1.0, 'gamma': 0.1} with a score of 0.81\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are {0} with a score of {1:0.2f}'.format(search.best_params_, search.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2261cf96",
   "metadata": {},
   "source": [
    "##### 4.5 Finales Modell mit optimierten Parametern\n",
    "Mit der Ausgabe aus <code>search.best_params_</code> ein neues Modell mit diesen Parametern auf den Trainingsdaten trainieren. Mit dem optimierten Modell die Vorhersagen auf den Testdaten treffen und dann evaluieren.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f7762b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7757\n",
      "Precision 0.8136\n",
      "Recall 0.7869\n",
      "F1 0.8\n"
     ]
    }
   ],
   "source": [
    "# Define model with optimized hyper-parameter\n",
    "opt_model = svm.SVC(C=1, gamma=0.1, kernel='rbf')\n",
    "# Fit model on training data\n",
    "opt_model.fit(X_train, y_train)\n",
    "# Predict test set with optimized model\n",
    "y_opt = opt_model.predict(X_test)\n",
    "# Evaluate model\n",
    "accuracy = round(accuracy_score(y_test, y_opt), 4)\n",
    "precision = round(precision_score(y_test, y_opt), 4)\n",
    "recall = round(recall_score(y_test, y_opt), 4)\n",
    "f1 = round(f1_score(y_test, y_opt), 4)\n",
    "print('Accuracy {}\\nPrecision {}\\nRecall {}\\nF1 {}'.format(accuracy, precision, recall, f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68c1363",
   "metadata": {},
   "source": [
    "### b) Phonetik & Phonologie\n",
    "\n",
    "**1.** Phonetik kann in drei Teildomänen unterteilt werden. Nennen Sie **eine Teildomäne** und erläutern Sie diese. \n",
    "\n",
    "**2.** Nennen Sie **einen** phonologischen Prozess.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30497e1",
   "metadata": {},
   "source": [
    "### c) Morphologie\n",
    "**1.** Geben Sie für den folgenden Satz die entsprechende Anzahl an Token und Types basierend auf **a) Wortformen** und **b) Lexemen** an. \n",
    "\n",
    ">Ist es der oder das Korpus und was ist mit Korpussen und Korpora ? \n",
    "\n",
    "**2.** Wie nennt man das kleinste Form-Bedeutungs-Paar, d. h. die konkrete Realisierung einer Bedeutung?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7feca8",
   "metadata": {},
   "source": [
    "### d) Semantik & Pragmatik\n",
    "**1.** Gegeben sind folgende Wortpaare:\n",
    "1. Käse, Tilsiter\n",
    "2. Moneten, Kröten\n",
    "3. Lachs, Fisch\n",
    "\n",
    "sowie folgende Relationen zwischen Konzepten:\n",
    "- Synonymie\n",
    "- Hypernymie\n",
    "- Homophonie\n",
    "- Hyponymie\n",
    "- Homonymie\n",
    "\n",
    "Geben Sie für jedes Wortpaar an, um welche Art von Relation es sich handelt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e10b02",
   "metadata": {},
   "source": [
    "**2.** Was ist eine deiktische Referenz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf69aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "62fd54318f6a2fa08be4eeaf7e2f88489af61833aae47133177ad67efbdf279b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
