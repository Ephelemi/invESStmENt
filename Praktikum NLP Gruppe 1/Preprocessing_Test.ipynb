{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 18:21:36.444010: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-24 18:21:36.707300: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-24 18:21:36.708720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 18:21:37.880696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-24 18:21:39.430210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-24 18:21:39.430921: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Stop word list from NLTK\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "stopword_list.append('@user')\n",
    "stopword_list.append('url')\n",
    "# Punctuation list from string\n",
    "puncts = string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kontraktionen exandieren -> Negation\n",
    "def decontract_phrase(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"shan\\'t\", \"shall not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not \", phrase)\n",
    "    phrase = re.sub(r\"\\'re \", \" are \", phrase)\n",
    "    phrase = re.sub(r\"\\'s \", \" is \", phrase)\n",
    "    phrase = re.sub(r\"\\'d \", \" would \", phrase)\n",
    "    phrase = re.sub(r\"\\'ll \", \" will \", phrase)\n",
    "    phrase = re.sub(r\"\\'t \", \" not \", phrase)\n",
    "    phrase = re.sub(r\"\\'ve \", \" have \", phrase)\n",
    "    phrase = re.sub(r\"\\'m \", \" am \", phrase)\n",
    "    \n",
    "    phrase = re.sub(r'\\s+', ' ', phrase)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(review):\n",
    "    doc = nlp(review)\n",
    "    lemma_text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return lemma_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER MP praise innovative publicsector think week Britain prosper 21st century embrace digital economy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_pipeline(review):\n",
    "    # Kontraktionen expandieren\n",
    "    review = decontract_phrase(review)\n",
    "    # Tokenisierung und Lemmatisierung\n",
    "    review = get_lemma(review)    \n",
    "    # Stopwords\n",
    "    review = ' '.join([token for token in review.split() if token.lower() not in stopword_list])\n",
    "    # Satzzeichen\n",
    "    review = ''.join([character for character in review if character not in puncts])\n",
    "    # Remove multiple whitespaces\n",
    "    review = re.sub(r'^\\s+', '', review)\n",
    "    review = re.sub(r' +', ' ', review)\n",
    "    review = re.sub(r'\\s+$', '', review)\n",
    "    \n",
    "    return review\n",
    "\n",
    "test = \".@USER @USER and @USER MP @USER praises the 'innovative #publicsector thinking' of @USER this week in @USER  If Britain is to prosper in the 21st century, it is through embracing the #digital economy URL URL\"\n",
    "preprocess_pipeline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER bakkt is doing what an ETF would have do...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER I can't ðŸ˜­ðŸ˜­ he is already 26</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER he is a psychic ainâ€™t he</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@USER @USER and @USER MP @USER praises the 'i...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER [Eric opens the door and runs to t...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Task\n",
       "0  @USER bakkt is doing what an ETF would have do...  OFF\n",
       "1                  @USER I can't ðŸ˜­ðŸ˜­ he is already 26  NOT\n",
       "2                     @USER he is a psychic ainâ€™t he  NOT\n",
       "3  .@USER @USER and @USER MP @USER praises the 'i...  NOT\n",
       "4  @USER @USER [Eric opens the door and runs to t...  OFF"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = pd.read_csv('./TrainAndDev-OLID/Praktikum_OLID_train.csv')\n",
    "dataset_dev = pd.read_csv('./TrainAndDev-OLID/Praktikum_OLID_dev.csv')\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10481 entries, 0 to 10480\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    10481 non-null  object\n",
      " 1   Task    10481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 163.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2620 entries, 0 to 2619\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    2620 non-null   object\n",
      " 1   Task    2620 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 41.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Are all columns defined?\n",
    "# Non-Null Count shows how many rows for each column are well-defined\n",
    "dataset_train.info()\n",
    "dataset_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10481 entries, 0 to 10480\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Text          10481 non-null  object\n",
      " 1   Task          10481 non-null  object\n",
      " 2   Preprocessed  10481 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 245.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2620 entries, 0 to 2619\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Text          2620 non-null   object\n",
      " 1   Task          2620 non-null   object\n",
      " 2   Preprocessed  2620 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 61.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_train['Preprocessed'] = dataset_train['Text'].apply(preprocess_pipeline)\n",
    "dataset_dev['Preprocessed'] = dataset_dev['Text'].apply(preprocess_pipeline)\n",
    "dataset_train.info()\n",
    "dataset_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Task</th>\n",
       "      <th>Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>. @USER @USER @USER @USER Fake conservatives a...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>fake conservative fake outrage budget betrayal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER You are sick in the head. This man...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>sick head man lose daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER best lead @USER &amp;amp; @USER most u...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>good lead amp underrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER The liberals can never handle the ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Task  \\\n",
       "0  . @USER @USER @USER @USER Fake conservatives a...  NOT   \n",
       "1  @USER @USER You are sick in the head. This man...  OFF   \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...  NOT   \n",
       "3  @USER @USER best lead @USER &amp; @USER most u...  NOT   \n",
       "4  @USER @USER The liberals can never handle the ...  OFF   \n",
       "\n",
       "                                        Preprocessed  \n",
       "0     fake conservative fake outrage budget betrayal  \n",
       "1                        sick head man lose daughter  \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...  \n",
       "3                            good lead amp underrate  \n",
       "4  liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()\n",
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10481, 14992)\n",
      "(2620, 6598)\n"
     ]
    }
   ],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X_cnt_train = cnt_vec.fit_transform(dataset_train['Preprocessed'])\n",
    "X_cnt_dev = cnt_vec.fit_transform(dataset_dev['Preprocessed'])\n",
    "print(X_cnt_train.shape)\n",
    "print(X_cnt_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train['char_count'] = dataset_train['Text'].apply(len)\n",
    "dataset_train['word_count'] = dataset_train['Text'].apply(lambda x: len(x.split()))\n",
    "dataset_train['Density'] = dataset_train['char_count'] / (dataset_train['word_count']+1)\n",
    "dataset_train['punctuation_count'] = dataset_train['Text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "dataset_train['Punct_Count_Ratio'] = dataset_train['punctuation_count'] / dataset_train['word_count']\n",
    "\n",
    "dataset_dev['char_count'] = dataset_dev['Text'].apply(len)\n",
    "dataset_dev['word_count'] = dataset_dev['Text'].apply(lambda x: len(x.split()))\n",
    "dataset_dev['Density'] = dataset_dev['char_count'] / (dataset_dev['word_count']+1)\n",
    "dataset_dev['punctuation_count'] = dataset_dev['Text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "dataset_dev['Punct_Count_Ratio'] = dataset_dev['punctuation_count'] / dataset_dev['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Task</th>\n",
       "      <th>Preprocessed</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>Punct_Count_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>. @USER @USER @USER @USER Fake conservatives a...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>fake conservative fake outrage budget betrayal</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>5</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER You are sick in the head. This man...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>sick head man lose daughter</td>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>4.642857</td>\n",
       "      <td>4</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...</td>\n",
       "      <td>134</td>\n",
       "      <td>21</td>\n",
       "      <td>6.090909</td>\n",
       "      <td>5</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER best lead @USER &amp;amp; @USER most u...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>good lead amp underrate</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER The liberals can never handle the ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...</td>\n",
       "      <td>90</td>\n",
       "      <td>15</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Task  \\\n",
       "0  . @USER @USER @USER @USER Fake conservatives a...  NOT   \n",
       "1  @USER @USER You are sick in the head. This man...  OFF   \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...  NOT   \n",
       "3  @USER @USER best lead @USER &amp; @USER most u...  NOT   \n",
       "4  @USER @USER The liberals can never handle the ...  OFF   \n",
       "\n",
       "                                        Preprocessed  char_count  word_count  \\\n",
       "0     fake conservative fake outrage budget betrayal         100          17   \n",
       "1                        sick head man lose daughter          65          13   \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...         134          21   \n",
       "3                            good lead amp underrate          55           9   \n",
       "4  liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...          90          15   \n",
       "\n",
       "    Density  punctuation_count  Punct_Count_Ratio  \n",
       "0  5.555556                  5           0.294118  \n",
       "1  4.642857                  4           0.307692  \n",
       "2  6.090909                  5           0.238095  \n",
       "3  5.500000                  6           0.666667  \n",
       "4  5.625000                  5           0.333333  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()\n",
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10481, 14994)\n",
      "(2620, 6600)\n"
     ]
    }
   ],
   "source": [
    "X_dense_train = X_cnt_train.toarray()\n",
    "X_dense_dev = X_cnt_dev.toarray()\n",
    "\n",
    "X_ling_train = np.vstack((dataset_train['Density'], \n",
    "                    dataset_train['Punct_Count_Ratio'])).T\n",
    "X_ling_dev = np.vstack((dataset_dev['Density'], \n",
    "                    dataset_dev['Punct_Count_Ratio'])).T\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_ling_scale_train = scaler.fit_transform(X_ling_train)\n",
    "X_ling_scale_dev = scaler.fit_transform(X_ling_dev)\n",
    "X_train = np.hstack((X_dense_train, X_ling_scale_train))\n",
    "X_dev = np.hstack((X_dense_dev, X_ling_scale_dev))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10481,)\n",
      "(2620,)\n",
      "[0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
      "[1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode Labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(dataset_train['Task'])\n",
    "y_dev = encoder.fit_transform(dataset_dev['Task'])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_train[1:100])\n",
    "print(y_dev[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 done\n",
      "Fold 2 done\n",
      "Fold 3 done\n",
      "Fold 4 done\n",
      "Fold 5 done\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "evaluation_list = list()\n",
    "\n",
    "for fold, [train, test] in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Get test data\n",
    "    X_test = X_train[test]\n",
    "    y_test = y_train[test]\n",
    "    # Define model\n",
    "    model = svm.LinearSVC()\n",
    "    # Fit model\n",
    "    model.fit(X_train[train], y_train[train])\n",
    "    # Predict test set with model\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate model\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 4)\n",
    "    precision = round(precision_score(y_test, y_pred), 4)\n",
    "    recall = round(recall_score(y_test, y_pred), 4)\n",
    "    f1 = round(f1_score(y_test, y_pred), 4)\n",
    "    # Append results\n",
    "    evaluation_list.append([fold+1, accuracy, precision, recall, f1])\n",
    "    print('Fold', fold+1, 'done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fold  Accuracy  Precision  Recall  F1-Score\n",
      "0    1    0.7258     0.5921  0.5398    0.5647\n",
      "1    2    0.7300     0.6050  0.5210    0.5599\n",
      "2    3    0.7290     0.5987  0.5398    0.5677\n",
      "3    4    0.7195     0.5825  0.5217    0.5505\n",
      "4    5    0.7400     0.6167  0.5551    0.5843\n",
      "5  AVG    0.7289     0.5990  0.5355    0.5654\n"
     ]
    }
   ],
   "source": [
    "# Export Results\n",
    "acc_avg = [i[1] for i in evaluation_list]\n",
    "prec_avg = [i[2] for i in evaluation_list]\n",
    "recall_avg = [i[3] for i in evaluation_list]\n",
    "f1_avg = [i[4] for i in evaluation_list]\n",
    "\n",
    "evaluation_list.append(['AVG',\n",
    "                        round(np.mean(acc_avg), 4),\n",
    "                        round(np.mean(prec_avg), 4),\n",
    "                        round(np.mean(recall_avg), 4),\n",
    "                        round(np.mean(f1_avg), 4)])\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_list, columns=['Fold', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "#evaluation_df.to_csv('./Wine_SVM_evaluation.csv', index=False, sep=';')\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 169 candidates, totalling 845 fits\n"
     ]
    }
   ],
   "source": [
    "# now create a GridSearchCV object and fit it to the data\n",
    "search = GridSearchCV(estimator=svm.SVC(),\n",
    "                      param_grid=param_grid,\n",
    "                      verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
