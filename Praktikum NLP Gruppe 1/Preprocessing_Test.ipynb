{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-24 18:41:28.294564: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-24 18:41:28.500204: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-24 18:41:28.501267: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-24 18:41:29.750901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-05-24 18:41:31.320784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-05-24 18:41:31.321649: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English spacy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Stop word list from NLTK\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "stopword_list.append('@user')\n",
    "stopword_list.append('url')\n",
    "# Punctuation list from string\n",
    "puncts = string.punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kontraktionen exandieren -> Negation\n",
    "def decontract_phrase(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"shan\\'t\", \"shall not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not \", phrase)\n",
    "    phrase = re.sub(r\"\\'re \", \" are \", phrase)\n",
    "    phrase = re.sub(r\"\\'s \", \" is \", phrase)\n",
    "    phrase = re.sub(r\"\\'d \", \" would \", phrase)\n",
    "    phrase = re.sub(r\"\\'ll \", \" will \", phrase)\n",
    "    phrase = re.sub(r\"\\'t \", \" not \", phrase)\n",
    "    phrase = re.sub(r\"\\'ve \", \" have \", phrase)\n",
    "    phrase = re.sub(r\"\\'m \", \" am \", phrase)\n",
    "    \n",
    "    phrase = re.sub(r'\\s+', ' ', phrase)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(review):\n",
    "    doc = nlp(review)\n",
    "    lemma_text = ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    return lemma_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USER MP praise innovative publicsector think week Britain prosper 21st century embrace digital economy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_pipeline(review):\n",
    "    # Kontraktionen expandieren\n",
    "    review = decontract_phrase(review)\n",
    "    # Tokenisierung und Lemmatisierung\n",
    "    review = get_lemma(review)    \n",
    "    # Stopwords\n",
    "    review = ' '.join([token for token in review.split() if token.lower() not in stopword_list])\n",
    "    # Satzzeichen\n",
    "    review = ''.join([character for character in review if character not in puncts])\n",
    "    # Remove multiple whitespaces\n",
    "    review = re.sub(r'^\\s+', '', review)\n",
    "    review = re.sub(r' +', ' ', review)\n",
    "    review = re.sub(r'\\s+$', '', review)\n",
    "    \n",
    "    return review\n",
    "\n",
    "test = \".@USER @USER and @USER MP @USER praises the 'innovative #publicsector thinking' of @USER this week in @USER  If Britain is to prosper in the 21st century, it is through embracing the #digital economy URL URL\"\n",
    "preprocess_pipeline(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@USER bakkt is doing what an ETF would have do...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER I can't ðŸ˜­ðŸ˜­ he is already 26</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER he is a psychic ainâ€™t he</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.@USER @USER and @USER MP @USER praises the 'i...</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER [Eric opens the door and runs to t...</td>\n",
       "      <td>OFF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Task\n",
       "0  @USER bakkt is doing what an ETF would have do...  OFF\n",
       "1                  @USER I can't ðŸ˜­ðŸ˜­ he is already 26  NOT\n",
       "2                     @USER he is a psychic ainâ€™t he  NOT\n",
       "3  .@USER @USER and @USER MP @USER praises the 'i...  NOT\n",
       "4  @USER @USER [Eric opens the door and runs to t...  OFF"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = pd.read_csv('./TrainAndDev-OLID/Praktikum_OLID_train.csv')\n",
    "dataset_dev = pd.read_csv('./TrainAndDev-OLID/Praktikum_OLID_dev.csv')\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10481 entries, 0 to 10480\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    10481 non-null  object\n",
      " 1   Task    10481 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 163.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2620 entries, 0 to 2619\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Text    2620 non-null   object\n",
      " 1   Task    2620 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 41.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Are all columns defined?\n",
    "# Non-Null Count shows how many rows for each column are well-defined\n",
    "dataset_train.info()\n",
    "dataset_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10481 entries, 0 to 10480\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Text          10481 non-null  object\n",
      " 1   Task          10481 non-null  object\n",
      " 2   Preprocessed  10481 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 245.8+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2620 entries, 0 to 2619\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Text          2620 non-null   object\n",
      " 1   Task          2620 non-null   object\n",
      " 2   Preprocessed  2620 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 61.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dataset_train['Preprocessed'] = dataset_train['Text'].apply(preprocess_pipeline)\n",
    "dataset_dev['Preprocessed'] = dataset_dev['Text'].apply(preprocess_pipeline)\n",
    "dataset_train.info()\n",
    "dataset_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Task</th>\n",
       "      <th>Preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>. @USER @USER @USER @USER Fake conservatives a...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>fake conservative fake outrage budget betrayal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER You are sick in the head. This man...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>sick head man lose daughter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER best lead @USER &amp;amp; @USER most u...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>good lead amp underrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER The liberals can never handle the ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Task  \\\n",
       "0  . @USER @USER @USER @USER Fake conservatives a...  NOT   \n",
       "1  @USER @USER You are sick in the head. This man...  OFF   \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...  NOT   \n",
       "3  @USER @USER best lead @USER &amp; @USER most u...  NOT   \n",
       "4  @USER @USER The liberals can never handle the ...  OFF   \n",
       "\n",
       "                                        Preprocessed  \n",
       "0     fake conservative fake outrage budget betrayal  \n",
       "1                        sick head man lose daughter  \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...  \n",
       "3                            good lead amp underrate  \n",
       "4  liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()\n",
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10481, 14992)\n",
      "(2620, 6598)\n"
     ]
    }
   ],
   "source": [
    "cnt_vec = CountVectorizer()\n",
    "X_cnt_train = cnt_vec.fit_transform(dataset_train['Preprocessed'])\n",
    "X_cnt_dev = cnt_vec.fit_transform(dataset_dev['Preprocessed'])\n",
    "print(X_cnt_train.shape)\n",
    "print(X_cnt_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train['char_count'] = dataset_train['Text'].apply(len)\n",
    "dataset_train['word_count'] = dataset_train['Text'].apply(lambda x: len(x.split()))\n",
    "dataset_train['Density'] = dataset_train['char_count'] / (dataset_train['word_count']+1)\n",
    "dataset_train['punctuation_count'] = dataset_train['Text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "dataset_train['Punct_Count_Ratio'] = dataset_train['punctuation_count'] / dataset_train['word_count']\n",
    "\n",
    "dataset_dev['char_count'] = dataset_dev['Text'].apply(len)\n",
    "dataset_dev['word_count'] = dataset_dev['Text'].apply(lambda x: len(x.split()))\n",
    "dataset_dev['Density'] = dataset_dev['char_count'] / (dataset_dev['word_count']+1)\n",
    "dataset_dev['punctuation_count'] = dataset_dev['Text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation)))\n",
    "dataset_dev['Punct_Count_Ratio'] = dataset_dev['punctuation_count'] / dataset_dev['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Task</th>\n",
       "      <th>Preprocessed</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>Density</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>Punct_Count_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>. @USER @USER @USER @USER Fake conservatives a...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>fake conservative fake outrage budget betrayal</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>5</td>\n",
       "      <td>0.294118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USER @USER You are sick in the head. This man...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>sick head man lose daughter</td>\n",
       "      <td>65</td>\n",
       "      <td>13</td>\n",
       "      <td>4.642857</td>\n",
       "      <td>4</td>\n",
       "      <td>0.307692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...</td>\n",
       "      <td>134</td>\n",
       "      <td>21</td>\n",
       "      <td>6.090909</td>\n",
       "      <td>5</td>\n",
       "      <td>0.238095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@USER @USER best lead @USER &amp;amp; @USER most u...</td>\n",
       "      <td>NOT</td>\n",
       "      <td>good lead amp underrate</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@USER @USER The liberals can never handle the ...</td>\n",
       "      <td>OFF</td>\n",
       "      <td>liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...</td>\n",
       "      <td>90</td>\n",
       "      <td>15</td>\n",
       "      <td>5.625000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Task  \\\n",
       "0  . @USER @USER @USER @USER Fake conservatives a...  NOT   \n",
       "1  @USER @USER You are sick in the head. This man...  OFF   \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ 'Schumer and Feinstein Go...  NOT   \n",
       "3  @USER @USER best lead @USER &amp; @USER most u...  NOT   \n",
       "4  @USER @USER The liberals can never handle the ...  OFF   \n",
       "\n",
       "                                        Preprocessed  char_count  word_count  \\\n",
       "0     fake conservative fake outrage budget betrayal         100          17   \n",
       "1                        sick head man lose daughter          65          13   \n",
       "2  ðŸ›‘ Truthfeed News ðŸ›‘ ðŸ‘‰ Schumer Feinstein Go HYST...         134          21   \n",
       "3                            good lead amp underrate          55           9   \n",
       "4  liberal never handle truth truth make head ðŸ’¥ ðŸ’¥...          90          15   \n",
       "\n",
       "    Density  punctuation_count  Punct_Count_Ratio  \n",
       "0  5.555556                  5           0.294118  \n",
       "1  4.642857                  4           0.307692  \n",
       "2  6.090909                  5           0.238095  \n",
       "3  5.500000                  6           0.666667  \n",
       "4  5.625000                  5           0.333333  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head()\n",
    "dataset_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10481, 14994)\n",
      "(2620, 6600)\n"
     ]
    }
   ],
   "source": [
    "X_dense_train = X_cnt_train.toarray()\n",
    "X_dense_dev = X_cnt_dev.toarray()\n",
    "\n",
    "X_ling_train = np.vstack((dataset_train['Density'], \n",
    "                    dataset_train['Punct_Count_Ratio'])).T\n",
    "X_ling_dev = np.vstack((dataset_dev['Density'], \n",
    "                    dataset_dev['Punct_Count_Ratio'])).T\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_ling_scale_train = scaler.fit_transform(X_ling_train)\n",
    "X_ling_scale_dev = scaler.fit_transform(X_ling_dev)\n",
    "X_train = np.hstack((X_dense_train, X_ling_scale_train))\n",
    "X_dev = np.hstack((X_dense_dev, X_ling_scale_dev))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10481,)\n",
      "(2620,)\n",
      "[0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0\n",
      " 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 1\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
      "[1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 1 1 1\n",
      " 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Encode Labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(dataset_train['Task'])\n",
    "y_dev = encoder.fit_transform(dataset_dev['Task'])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_dev.shape)\n",
    "print(y_train[1:100])\n",
    "print(y_dev[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 done\n",
      "Fold 2 done\n",
      "Fold 3 done\n",
      "Fold 4 done\n",
      "Fold 5 done\n",
      "Fold 6 done\n",
      "Fold 7 done\n",
      "Fold 8 done\n",
      "Fold 9 done\n",
      "Fold 10 done\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "evaluation_list = list()\n",
    "\n",
    "for fold, [train, test] in enumerate(kfold.split(X_train, y_train)):\n",
    "    # Get test data\n",
    "    X_test = X_train[test]\n",
    "    y_test = y_train[test]\n",
    "    # Define model\n",
    "    model = svm.LinearSVC()\n",
    "    # Fit model\n",
    "    model.fit(X_train[train], y_train[train])\n",
    "    # Predict test set with model\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Evaluate model\n",
    "    accuracy = round(accuracy_score(y_test, y_pred), 4)\n",
    "    precision = round(precision_score(y_test, y_pred), 4)\n",
    "    recall = round(recall_score(y_test, y_pred), 4)\n",
    "    f1 = round(f1_score(y_test, y_pred), 4)\n",
    "    # Append results\n",
    "    evaluation_list.append([fold+1, accuracy, precision, recall, f1])\n",
    "    print('Fold', fold+1, 'done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold  Accuracy  Precision  Recall  F1-Score\n",
      "0     1    0.7293     0.6000  0.5376    0.5671\n",
      "1     2    0.7128     0.5692  0.5347    0.5514\n",
      "2     3    0.7366     0.6151  0.5405    0.5754\n",
      "3     4    0.7300     0.6047  0.5188    0.5585\n",
      "4     5    0.7319     0.5988  0.5623    0.5800\n",
      "5     6    0.7319     0.6088  0.5188    0.5603\n",
      "6     7    0.7252     0.5872  0.5565    0.5714\n",
      "7     8    0.7195     0.5815  0.5275    0.5532\n",
      "8     9    0.7433     0.6250  0.5507    0.5855\n",
      "9    10    0.7319     0.6019  0.5478    0.5736\n",
      "10  AVG    0.7292     0.5992  0.5395    0.5676\n"
     ]
    }
   ],
   "source": [
    "# Export Results\n",
    "acc_avg = [i[1] for i in evaluation_list]\n",
    "prec_avg = [i[2] for i in evaluation_list]\n",
    "recall_avg = [i[3] for i in evaluation_list]\n",
    "f1_avg = [i[4] for i in evaluation_list]\n",
    "\n",
    "evaluation_list.append(['AVG',\n",
    "                        round(np.mean(acc_avg), 4),\n",
    "                        round(np.mean(prec_avg), 4),\n",
    "                        round(np.mean(recall_avg), 4),\n",
    "                        round(np.mean(f1_avg), 4)])\n",
    "\n",
    "evaluation_df = pd.DataFrame(evaluation_list, columns=['Fold', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])\n",
    "#evaluation_df.to_csv('./Wine_SVM_evaluation.csv', index=False, sep=';')\n",
    "print(evaluation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 13)\n",
    "tol_range = np.logspace(-9, 3, 13)\n",
    "param_grid = dict(tol=tol_range, C=C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 169 candidates, totalling 845 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/jonas/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# now create a GridSearchCV object and fit it to the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39msvm\u001b[39m.\u001b[39mLinearSVC(),\n\u001b[1;32m      3\u001b[0m                       param_grid\u001b[39m=\u001b[39mparam_grid,\n\u001b[1;32m      4\u001b[0m                       verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[1;32m    818\u001b[0m         )\n\u001b[1;32m    819\u001b[0m     )\n\u001b[0;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m         clone(base_estimator),\n\u001b[1;32m    824\u001b[0m         X,\n\u001b[1;32m    825\u001b[0m         y,\n\u001b[1;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[1;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[1;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[1;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[1;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[1;32m    832\u001b[0m     )\n\u001b[1;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[1;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[1;32m    835\u001b[0m     )\n\u001b[1;32m    836\u001b[0m )\n\u001b[1;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    843\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     62\u001b[0m )\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[1;32m   1049\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1051\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[1;32m   1052\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1055\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    863\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[1;32m    865\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    781\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[0;32m--> 782\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[1;32m    783\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    785\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[1;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/joblib/parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    260\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 263\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    264\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/utils/parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/svm/_classes.py:274\u001b[0m, in \u001b[0;36mLinearSVC.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m check_classification_targets(y)\n\u001b[1;32m    272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n\u001b[0;32m--> 274\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_, n_iter_ \u001b[39m=\u001b[39m _fit_liblinear(\n\u001b[1;32m    275\u001b[0m     X,\n\u001b[1;32m    276\u001b[0m     y,\n\u001b[1;32m    277\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mC,\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintercept_scaling,\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    281\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty,\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual,\n\u001b[1;32m    283\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[1;32m    285\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[1;32m    286\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[1;32m    287\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmulti_class,\n\u001b[1;32m    288\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss,\n\u001b[1;32m    289\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    290\u001b[0m )\n\u001b[1;32m    291\u001b[0m \u001b[39m# Backward compatibility: _fit_liblinear is used both by LinearSVC/R\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m# and LogisticRegression but LogisticRegression sets a structured\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m# `n_iter_` attribute with information about the underlying OvR fits\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# while LinearSVC/R only reports the maximum value.\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m n_iter_\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/svm/_base.py:1224\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1221\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m   1223\u001b[0m solver_type \u001b[39m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1224\u001b[0m raw_coef_, n_iter_ \u001b[39m=\u001b[39m liblinear\u001b[39m.\u001b[39;49mtrain_wrap(\n\u001b[1;32m   1225\u001b[0m     X,\n\u001b[1;32m   1226\u001b[0m     y_ind,\n\u001b[1;32m   1227\u001b[0m     sp\u001b[39m.\u001b[39;49misspmatrix(X),\n\u001b[1;32m   1228\u001b[0m     solver_type,\n\u001b[1;32m   1229\u001b[0m     tol,\n\u001b[1;32m   1230\u001b[0m     bias,\n\u001b[1;32m   1231\u001b[0m     C,\n\u001b[1;32m   1232\u001b[0m     class_weight_,\n\u001b[1;32m   1233\u001b[0m     max_iter,\n\u001b[1;32m   1234\u001b[0m     rnd\u001b[39m.\u001b[39;49mrandint(np\u001b[39m.\u001b[39;49miinfo(\u001b[39m\"\u001b[39;49m\u001b[39mi\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mmax),\n\u001b[1;32m   1235\u001b[0m     epsilon,\n\u001b[1;32m   1236\u001b[0m     sample_weight,\n\u001b[1;32m   1237\u001b[0m )\n\u001b[1;32m   1238\u001b[0m \u001b[39m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[39m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[39m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[39m# srand supports\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m n_iter_max \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(n_iter_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now create a GridSearchCV object and fit it to the data\n",
    "search = GridSearchCV(estimator=svm.LinearSVC(),\n",
    "                      param_grid=param_grid,\n",
    "                      verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
